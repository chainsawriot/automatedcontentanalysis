[
["index.html", "Automated Content Analysis 1 Prerequisites", " Automated Content Analysis Chung-hong Chan 2020-01-26 1 Prerequisites The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "2 Introduction", " 2 Introduction When I was a high-school student, biology was my favourite subject. Of course, I need to have the biology textbook to study and my school had assigned me to buy one. My older brother used the same textbook but an outdated edition and thus I didn’t need to buy one, I just used his book. My classmates were using the newer edition, but I enjoyed reading the old edition from my brother. That book had over 1,000 pages and was completely in black and white. Students and teachers alike spotted me reading that book almost all the time at the corner of the spectator stand. Why do I need to bring this little irrelevant story up? Francis Crick, James Watson and the oft-ignored female post-graduate student Rosalind Franklin discovered the double-helix structure of DNA in 1953. By the time I was reading the biology textbook printed in the early 90s, it was almost 40 years ago. The biology textbook of the period had an introduction to the double helix structure of DNA, as you can imagine, in a boring way. I would say the discover of the double helix structure is probably one of the most important discovery in science, the textbook of the period did not describe it as a “hype”. Instead, it was described as if it was as boring as counting sheep. When something was written in the textbook, it is instantly becoming uncool. But still, why do I need to bring this little irrelevant story up? Some would dispute this, but the world first automated content analysis system is probably the General Inquirer system. The original paper about the system was published in 1962 by the late Philip J. Stone and his colleagues at the Harvard Laboratory of Social Relations. The General Inquirer (GI) uses a method — that is called “dictionary-based method” now — to quantify the characteristics of a piece of text. At the time of my writing, it is 2020 now. The world first automayed content analysis was 58 years old. If there is still an innovative factor of using a 58-year old technology and hype about it, it is actually a bit absurd. Although mew dictionaries are developed almost very month, many communication researchers — myself included — are doing automated content analysis the way very similar to Philip J. Stone and Co. did back in 1962. The discouse about automated content analysis should be very similar to way I read about the double helix structure when I was a high-school student in the mid 90s: it should be boring and uncool. "],
["the-whole-game.html", "3 The whole game", " 3 The whole game We are going to use an example to illustrate the whole process of a typical automated content analysis scenario. In this example, we start with a simple research question or hypothesis. Let’s say I want to reproduce a very famous analysis of Donald Trump’s tweets. This example is very well-known in the data science scene, probably because one of the authors of the tidytext package used this example to demonstrate the power of his package (and made him on the television). The research question is very simple: are tweets from Donald Trump’s twitter account tweeted using an iPhone less angry than those tweeted using an Android phone? There are many elements to unpack in the above paragraph. The utmost important issue is that, all automated content analysis project should have hypotheses to test or research questions to answer. If a project without hypotheses or research questions, it can hardly be called automated content analysis (see Chapter 2 for longer discussion). Then we can think of the context we are interested to analyze (Donald Trump and his Twitter), operationalization of variables (what is angry?), data collection plan and data analysis strategy. In this book, however, we are not going to focus on 1) how to form hypotheses or research questions and 2) how to collect your (text) data. The reason for excluding the former is that, it needs to be supported by communication theories. As a book that is intented as an research methods book, it is probably a bit too much. For the latter, the exclusion of it is due to the fact that there are good papers and textbooks available. The book chapter by Liang &amp; Zhu (2017) is probably a good start. Simon Munzert et al (2015) ’s Automated Data Collection with R is an in-depth manual. In the companion website of this book, you can find the tweets from Donald Trump’s tweet account before he assumes duty as the president of the United States. The data looks like this: require(tidyverse) ## Loading required package: tidyverse ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.3 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() require(quanteda) ## Loading required package: quanteda ## Package version: 1.5.2 ## Parallel computing: 2 of 4 threads used. ## See https://quanteda.io for tutorials and examples. ## ## Attaching package: &#39;quanteda&#39; ## The following object is masked from &#39;package:utils&#39;: ## ## View require(rio) ## Loading required package: rio ## ## Attaching package: &#39;rio&#39; ## The following object is masked from &#39;package:quanteda&#39;: ## ## convert trump_tweets &lt;- import(&#39;./data/trump.json&#39;) %&gt;% as_tibble trump_tweets ## # A tibble: 17,936 x 7 ## source text created_at retweet_count favorite_count is_retweet id_str ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 Twitte… Heads… Mon Dec 3… 20519 74566 FALSE 10798… ## 2 Twitte… ....S… Mon Dec 3… 17027 63013 FALSE 10798… ## 3 Twitte… It’s … Mon Dec 3… 29355 125931 FALSE 10797… ## 4 Twitte… I’m i… Mon Dec 3… 30742 131151 FALSE 10797… ## 5 Twitte… I’m i… Mon Dec 3… 1123 4217 FALSE 10797… ## 6 Twitte… I am … Mon Dec 3… 25252 111582 FALSE 10797… ## 7 Twitte… I cam… Mon Dec 3… 21960 90883 FALSE 10797… ## 8 Twitte… .....… Mon Dec 3… 15081 72353 FALSE 10797… ## 9 Twitte… ...I … Mon Dec 3… 22000 100819 FALSE 10797… ## 10 Twitte… If an… Mon Dec 3… 17379 79095 FALSE 10797… ## # … with 17,926 more rows "],
["what-is-automated-content-analysis.html", "4 What is automated content analysis? 4.1 What is Content analysis 4.2 What gets automated? 4.3 Best practices", " 4 What is automated content analysis? 4.1 What is Content analysis As a super express summary of Krippendorff and Neuendorf. But don’t do it like Leetaru! 4.2 What gets automated? What the current literature says about ACA. 4.3 Best practices Validation Contrasting Confirmatory / exploratory Methodological transparency "],
["creating-gold-standard-validation.html", "5 Creating gold standard &amp; validation", " 5 Creating gold standard &amp; validation avocate a workflow of making gold standard first! "],
["typical-automated-content-analytic-methods.html", "6 Typical automated content analytic methods 6.1 dictionary-based method 6.2 topic-model", " 6 Typical automated content analytic methods 6.1 dictionary-based method how it works off-the-shelf creating domain specific one 6.1.1 validation suggestions 6.2 topic-model 6.2.1 validation daniel maier / Chang "],
["advance-topics.html", "7 Advance topics 7.1 bag-of-embeddings 7.2 semantic network 7.3 machine learning 7.4 multimodal analysis 7.5 crosslingual analysis", " 7 Advance topics 7.1 bag-of-embeddings 7.2 semantic network 7.3 machine learning 7.4 multimodal analysis 7.5 crosslingual analysis "],
["references.html", "References", " References "]
]
